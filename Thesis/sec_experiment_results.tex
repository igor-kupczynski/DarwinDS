DARWIN is a new method and according to knowledge of the author and the
supervisor haven't been implemented before. Experiments are
designed to check if the method is working at all, what parameters are important
for the method and what should be their reasonable default values. To make
results repeatable the DM is mocked. Noise in his or her decisions is
simulated. Unless uncertainty is involved comparisons to exact optimal
solution are provided. In tests involving uncertainty results are compared to
supposed utility function optimisation. 

\section{The environment}

All tests were conducted on a personal computer with 64bit Intel
processor. RAM size on the machine is 3GB. 64bit Linux operating system was
used. The Java Virtual Machine was in version 1.6.0\_18 and Scala 2.8.0. JVM
was run with options \texttt{-Xms768m -Xmx768m} thus setting memory available
for application to 768MB. Tests were performed through CLI batch interface.

Test framework is available in order to automate the experiment process. All
experiments were repeated at least thirteen times. Data analysis and chart
generation was performed using an R environment [ref]. The framework is a
combination of Python [ref] and Bash [ref] code communicating with main DARWIN
code and with the modules written in R.

\section{The Decision Maker}

Experiments were repeated many times so to make it possible it was essential
to mock the DM. This was done to assure /TODO/


\section{Problem selection}

Area of interest for Multi-Objective Optimisation is huge and consists of many
potential problems to be solved. There are multi-criteria versions of
classical problems, like minimal spanning tree~[ref], traveling salesman
problem (TSP)~[ref] or knapsack problem~[ref] as well as artificially
generated ones --- like the DTLZ problems [ref]. Some of them are interesting
because of their real-life applications while the other are good for
experimenting and testing purposes.

It is worth noting that the ordinary single-criterion versions of the problems
can be easy to solve. However in multi-criteria setting one has to infer the
decision maker preferences and approximate supposed utility function
correctly. The challenge here is not to build the best optimisation algorithm
for all the problems (what is impossible according to no-free-lunch theorem
[ref]) but rather a framework for preference information extraction.

Introducing uncertainty model as intervals is a new concept. For this reason
the uncertainty was added to the problems described above.

The experiments were performed using following problems:
\begin{description}
  \item{\textbf{Two-criteria binary knapsack problem}}
    \begin{align*}
      & \bar{x} = [x_1, x_2, \dots, x_{300}]  \\
      & x_i \in \{0, 1\};  \hspace{0.5cm} i = 1, 2, \dots, 300 \\
      & \textit{max}\text{ value$_1$:} \hspace{0.2cm} \bar{a_1} \cdot \bar{x} \\
      & \textit{max}\text{ value$_2$:} \hspace{0.2cm} \bar{a_2} \cdot \bar{x} \\
      & \textit{subject to:} \\
      & \hspace{0.5cm} \text{weight:} \hspace{0.2cm} \bar{w} \cdot \bar{x} \leq b
    \end{align*}
    
    Where $\bar{x}$ is a vector of items to be chosen. The problem is binary,
    so each $x_i \in \bar{x}$ can be either selected ($x_i = 1$) or not($x_i
    = 0$). There are two criteria: value$_1$ and value$_2$. Each one is a sum
    of items multiplied with associated weights (vector $\bar{a_1}$ and
    $\bar{a_2}$).

    Knapsack constraint is given. One can choose items up to a certain weight
    ($b$). There is a vector of weights associated with each item
    ($\bar{w}$). The limit is defined that it is possible to choose about
    $2/3$ of the items.

    Weights ($\bar{a_1}, \bar{a_2}, \bar{w}$) are uniformly distributed
    vectors of values in $[0,10)$ interval.


  \item{\textbf{Two-criteria continuous knapsack problem}}
    \begin{align*}
      & \bar{x} = [x_1, x_2, \dots, x_{300}]  \\
      & x_i \in [0, 1);  \hspace{0.5cm} i = 1, 2, \dots, 300 \\
      & \textit{max}\text{ value$_1$:} \hspace{0.2cm} \bar{a_1} \cdot \bar{x} \\
      & \textit{max}\text{ value$_2$:} \hspace{0.2cm} \bar{a_2} \cdot \bar{x} \\
      & \textit{subject to:} \\
      & \hspace{0.5cm} \text{weight:}  \hspace{0.2cm} \bar{w} \cdot \bar{x} \leq b
    \end{align*}

    Continuous version of the knapsack problem. Description given for the
    binary version also applies here. The only difference is that now item can
    be partially selected ($\forall_{x_i \in \bar{x}} x_i \in [0, 1)$). 

  \item{\textbf{Three-criteria binary knapsack problem}}
    \begin{align*}
      & \bar{x} = [x_1, x_2, \dots, x_{300}]  \\
      & x_i \in \{0, 1\};  \hspace{0.5cm} i = 1, 2, \dots, 300 \\
      & \textit{max}\text{ value$_1$:} \hspace{0.2cm} \bar{a_1} \cdot \bar{x} \\
      & \textit{max}\text{ value$_2$:} \hspace{0.2cm} \bar{a_2} \cdot \bar{x} \\
      & \textit{max}\text{ value$_3$:} \hspace{0.2cm} \bar{a_3} \cdot \bar{x} \\
      & \textit{subject to:} \\
      & \hspace{0.5cm} \text{weight:} \hspace{0.2cm} \bar{w} \cdot \bar{x} \leq b
    \end{align*}

    Two-criteria problems can be easily visualised and analysed, however in
    real-life applications there is often a need for three and more
    criteria. There is a moving from two to multiple, so it is worth comparing
    the results achieved on the three-criteria knapsack problem with its
    two-criteria counterpart. 

  \item{\textbf{Three-criteria DTLZ problem generated using constraint surface
    approach}}

  \item{\textbf{Two-criteria production problem with uncertainty} (described in
    presentation given on ... [ref])}

  \item{\textbf{Tree-criteria DTLZ7 problem with uncertainty}}

  \item{\textbf{Four-criteria DTLZ1 problem with uncertainty}}

  \item{\textbf{Ten-criteria DTLZ1 problem with uncertainty}}

\end{description}

\section{No uncertainty}

\section{The importance of parameters}

\section{Noise in the DM's decisions}

\section{Uncertainty}

\section{Conclusions}

